What is Docker : 
    tool that helps you package your application and everything it needs
    into a neat and portable box called container 
    (so it runs the same anywhere)

without docker : 
    app works on your system, but not on another (due to different OS
    library versions, etc.).
    managing dependencies is hard
    onboarding new developers is slow

with docker :   
    package your app + env  -> consitent and isolated
    works on my machine -> problem is gone
    easy to deploy and scale 

key concepts : 
    image(recipe of cake) : blueprint of your app (includes OS, code, dependecies)
    container(actual cake made by the recipe) : running instance of an image
    dockerfile : script to build an image 
    docker engine : software that runs containers 
    docker hub : public registry of docker images 
    volume : persistent storage for containers
    network : virtual network to allow containers to talk to each other 

docker architecture : 

    your application 
            |
            v 
    docker image  <- built using dockerfile 
            |
            v
    docker container <- running instance 
            |
            v
    docker engine (Daemon)
            |
            v
    host machine (linux/windows/macOS)

basic script for Dockerfile : 
    step 1 : use node.js base image 
        FROM node:18 
    step 2 : set working directory inside container 
        WORKDIR /app 
    step 3 : copy files 
        COPY package*.json ./ 
        RUN npm install 
        COPY . .
    step 4 : Expose port and run 
        EXPOSE 3000
        CMD ["node", "app.js"]

RUN it : 

    docker build -t my-node-app .

    docker run -p 3000:3000 my-node-app 

common docker CLI commands : 

    docker build -t name . 
        build image from dockerfile 
    
    docker images 
        list available images 
    
    docker run name 
        run a container from image 
    
    docker run game 
        run a container from image 
    
    docker ps 
        showing running containers 
    
    docker ps -a 
        show all containers (stopped + running)
    
    docker stop <id> 
        remove a container 
    
    docker rm <id> 
        remove a container 
    
    docker rmi <image> 
        remove an image 
    
    docker logs <id> 
        view container logs 
    
    docker exec -it <id> bash 
        open shell inside container 
    
docker volumes : 

    #create volume 
    docker volume create mydata 


    # use it in a container 
    docker run -v mydata:/app/data my-image 

    (
        in this command mydata is created (if not already present)
        docker mounts this volume into container at the location /app/data
        any files the app writes to /app/data are stored in the volume, not 
        inside the container filesystem 
        even if you delete the continer the data in mydata persists 

    )

docker networks : 

    containers can communicate via internal docer network 

    cmds : 

    docker network create mynet 
        create a virtual bridge network named mynet 
        think of it as a private LAN for docker containers 

    docker run --network=mynet --name=db my-db-image 
        starts a container from my-db-image 
        assigns it the name db 
        connects it to the mynet network 

    docker run --network=mynet --name=api my-api-image 
        starts another container from my-api-image 
        assigns it the name api 
        also connects it to the same mynet network 

    (
        docker bydefault isolates container, so they can't talk 
        to each other by hostname
    )

docker compose (multi-container setup)

    create docker-compose.yml 

    version : '3.8

    services:
        backend:
            build: .
            ports:
              - "3000:3000"
        
        mongo:
            image: mongo
            ports:
              -"27017:27017"
        
        run it all : 

            docker-compose up

docker hub - image registry 

    search https://hub.docker.com 

    pull : 

        docker pull nginx
        docker run -p 80:80 nginx 

    
    when to use docker 
    
when to use docker 
    use docker when :
        you dont want to ship code with consistent env 
        you work on microservices 
        you need isolation
        you're deploying to cloud or CI/CD 
    
    avoid if : 
        you're doing simple scripting or local-only tasks 
        you don't want to learn new tooling yet (steep leaning curve).
    

real life ex. 

    imagine you are building a pyton api with postgreSQL DB. 

    you want : 

        one container for python backend 
        one for postgreSQL 
        both isolated but connected 
    
    docker makes this easy via docker compose 

best practices : 
    
    keep Dockerfile minimal 
    use .dockerignore (like .gitignore)
    don;t store sensitive env vars in Dockerfile 
    use volumes for databases or large data 
    tag your images (myapp:v1)

lifecycle summary 
    
    grapj TD 
    
    (a) [write Dockerfile] --> (b) [build image]
    (b) --> (c) [run container]
    (c) --> (d) [app routing]
    (d) --> (e) [stop container]
    (e) --> (f) [remove container or image]

what is docker engine = core of docker 

    3 main parts : 

        docker CLI                      <- you type commands here 
            sends your request (docker run ..) as an http api 
            req to the docker Daemon(dockerd), usually over a 
            Unix socket (/var/run/docker.sock)

            |
            v 

        docker Daemon                   <- core process (dockerd)
        -manages containers, images
        -talks to containerd    
            it's the main bg process (like a server) that handles all docker requests
            parses your command 
            checks if the image exists locally if not it pulls from docker hub
            sends the container creation task to containerd 

            |
            v

        containerd                      <- Manages low-level container lifecycle 
        (open-source project)
            a lightweight container runtime that manages container lifecycle : start, stop,
            pause, snapshot etc 

            containerd was originally part of Docker but is now an independent CNCF project 
            
            talks to lower-level runtimes (like runc) to actually launch containers 
            manages file system snapshots (image layers)
            handles image management, containers execution


            receives instructions from docker daemon (dockerd)
            manages the entire lifecycle of a container 
                create 
                start 
                stop 
                pause 
                delete 
            works with image layers (snapshots)
            delegates actual container creation to runc (low-level runtime)

            what it does : 

                docker run --name my-app nginx 

                1. docker CLI sends this to dockerd 

                POST /containers/create
                {
                    "Image" : "nginx",
                    "Name" : "my-app"
                }

                2. dockerd parses this and talks to containerd 

                containerdClient.NewContainer(ctx, containerOpts)

                then containerd : 

                    pulls image (if not present) from docker hub using its own image service 
                    creates a snapshot of the image filesystem 
                        uses overlayFS (more below) creates a container task, but doesn't run it yet 
                    
                    creates a container task but doesn't run it yet 
                    finally containerd calls runc to launch the conatainer with 
                        namespaces 
                        cgroups 
                        volumes 
                        networking
                        entrypoint(nginx in this case)
                
                filesystem snapshots - what containerd manages 

                FROM node:18    # base layer 
                COPY . .        # adds new layer 
                RUN npm install # adds another layer 

                Each layer is a read-only snapshot 

                containerd uses : 
                    overlayFS (in linux) : to merge multiple layers into one unified view 
                    adds a read-write top layer for the container to make temp changes
                
                this gives : 

                    container filesystem = base layers (read-only) + RW top layer 

                    if the container writes to /app/data/file.txt, it goes in RW layer only.

                containerd - runc (handoff)

                after setting up the file system and configs, containerd finally calls runc with a JSON file 
                (config.json) which decribes : 

                    linux namespaces 
                    cgroups limits 
                    rootfs path 
                    user/group
                    entrypoint (like nginx or node)

                    runc uses this to start a new process in an isolated container environment 

            



            |   
            v

        runc (runtime)                  <- Launches containers using linux namespaces 
            it's a low-level container runtime that talks direclty 
            to the linux kernel to start containers 

            runc is the default runtime in docker and comliant with 
            the open container initiative (OCI).

            what it does : 

                runc tells the linux kernel :

                create a new isolated process 
                use linux namespace (explained below)
                apply resources limits via cgroups 
                create a fake root filesystem using chroot 
                blind mount necessary volumes
                set hostname, network interface 
                start the app like (node app.js)
            
            so the container is really just 

                a normal linux process with fencing (namespaces) and 
                some handcuffs(cgroups)

        Linux kernel feature (namespaces + cgroups)

        Namespaces - isolation 

        namespaces isolate what a process can see 

        each container gets its own 

        pid: own process tree (can't see host PIDs)
        net: own network stack (IP, routing, interfaces)
        mnt: own filesystem mounts 
        uts: own hostname 
        ipc: own inter-process communication 
        user: own user and groups (root inside container â‰  root on host)

        cgroups - resource limiting 

            control groups limit how much CPU, RAM, or I/O a container can use 
            
            docker run --memory=512m -cpus=1 my-image 

                this craetes cgroups rules : 
                    no more than 512MB RAM 
                    only 1 CPU core 
            
        chroot + mount namespace - fake filesystem 

            the container is started with a root filesystem made of your docker image 
                docker sets up a chroot jail : inside this jail, / is the root of the image 
                so even if the container process accesses /etc/passwd, it sees the 
                one inside the container image, not the host's 
        

        each container (by default) gets:
            a private IP address from the docker bridge network (usually 172.17.0.x)
            a virtual Ethernet interface (eth0)
            Gateway to communicate with other containers and internet 

        
        docker does this by : 
            creating a linux bridge (docker0)
            for each container adds a veth(virtual ethernet pair)
                adds a veth (virtual Ethernet pair)
                one end goes inside container(eth0)
                other end connects to the bridge(docker0)

            ex. 
                docker exec -it my-container ip addr 

                shows ->  eth0 : 172.17.0.2

docker DNS : no need for IPs 

internal DNS 
    when containers are in the same custom network, docker sets up 
    an internal DNS resolver 

    so if a container is named db, other container can access it with 
    db:port 

    docker run --network=mynet --name=db my-db-image  

    docker run --network=mynet --name=api my-api-image 

    inside api container : 

        const DB_URL = 'mongodb://db:27017/products';

    no IP needed docker's internal DNS resolves db->its private IP 



internals (low-level):

docker run my-image 

    docker CLI sends req to dockerd via HTTP API 

    dockerd 
        parses image and container config 
        talks to containerd to create a container instance 
    
    containerd uses runc (default runtime) to: 
        set up linux namespaces (PID, NET, MOUNT, etc)
        apply cgroups (resource limits)
        start a process inside a chroor-like environment



where Host machine live : 
    The host is your actual machine (windows/macos/linux)
    it runs docker engine 
    in docker desktop, docker runs inside a lightweight linux VM 
    using hyperkit or WSL2

so : 
    on linux docker runs natively 
    on windows/macos docker creats a virtual linux kernel as the real host
    where containers run.

real world analogy : 

    kubernetes = airport air traffic control (manages planes)
    container = airplane engines (actually run the plane)
    runc = Engine parts that turn commands into movement
    linux kernel = the laws of physics (isolation, process control)


~~~~~~~~~~~~~~~~~~~~~~~~~~~~kubernetes~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

What is kubernetes : 
    open source container orchestration platform

Major components : 
    pods
    service (internal, external)
    ingress routing pulic requests
    deployment 
    stateful set (deployment of DB(usually outside k8s))
    config maps (to map different services)
    secrets (to add env api key)
    volumns (manage persistent state)


worker machine in k8s : 

    3 node(node is physical machine) processes 
        kubelet : primary agent that runs on each worker node, communicate with control plane, manages pod lifecycle, reports node health, pulls pod specs, container management 

        kube proxy : network proxy, network component that handles traffic routing, service discovery, load balancing, network rules, inter-pod communication

        container runtime :  the software that actually runs container(containerd, docker, CRI-O), container Execution, image management, resource isolation, container monitoring 

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Master Node (Control Plane) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  kube-apiserver â”‚â—„â”€â”€â–ºâ”‚      etcd       â”‚             â”‚
â”‚  â”‚   (port 6443)   â”‚    â”‚  (key-value DB) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                                             â”‚
â”‚           â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ kube-scheduler  â”‚    â”‚   controller-   â”‚             â”‚
â”‚  â”‚ (pod placement) â”‚    â”‚    manager      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ (HTTPS API calls)
                           â–¼
                    Worker Nodes


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Worker Node â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”‚
â”‚  â”‚kubelet  â”‚â—„â”€â”€â–ºâ”‚ kube-proxy   â”‚          â”‚
â”‚  â”‚         â”‚    â”‚              â”‚          â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â”‚
â”‚       â”‚                                   â”‚
â”‚       â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚ container       â”‚                      â”‚
â”‚  â”‚ runtime         â”‚                      â”‚
â”‚  â”‚ (containerd)    â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â”‚       â”‚                                   â”‚
â”‚       â–¼                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚   Pod 1         â”‚   Pod 2              â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ â”‚Container A  â”‚ â”‚ â”‚Container B  â”‚      â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     
master machine(brain of kubernetes cluster) in k8s : 

4 main process : 
    kube-apiserver - the entry point : the front door of kubernetes all requests go through here, api gateway(handles all REST API requests (kubectkm kubelet, etc)), authentication and authorization(validates who can do what), request processing(validates and processes all cluster operations), communication hubs(all components talk through the API server) 

    ex . kubectl create deployment nginx --image-ngix 
         kubectl get pods 
         kubeclt delete service my-service 

         all these become HTTP requests to kube-apiserver on port 6443

    etcd : The cluster database, distributed key-value store that holds all cluster data, cluster data (current state of all resources(pods, services, deployments)), configuration data(configmaps, secret, node information), metadata(lables, annotations, resources definitions)

    it's the source of truth for the entire cluster  

    kube-scheduler - The pod placement engine, decides which node should run each pod, watches api server(looks for pods without assigned node), evaluates nodes(checks available resources, contraints, policies), makes placement decision(assigns pods to best-fit nodes), update etcd(stores the scheduling decision)

    #scheduling factors : 
        CPU/Memory availability
        node labels and taints 
        pod affinity/anti-affiity rules 
        resources requests and limits 
        hardware contraints (GPU, SSD, etc.) 

    Kube-controller-manager - the state manager : collection of controllers that ensures desired state, delpoyments controller(manages replicaSets and rolling updates), replicaSet controller(ensures correct number of pod replicas), node controller (monitors node health and handles failure)
    service controller : manages load balances and endpoints 

    ex. if you want 3 nginx pods but only 2 are running replicaset controller will create 1 more pod to match desired state 





â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Master Node (Control Plane) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚  kube-apiserver â”‚â—„â”€â”€â–ºâ”‚      etcd       â”‚             â”‚
â”‚  â”‚   (port 6443)   â”‚    â”‚  (key-value DB) â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚           â”‚                                             â”‚
â”‚           â–¼                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ kube-scheduler  â”‚    â”‚   controller-   â”‚             â”‚
â”‚  â”‚ (pod placement) â”‚    â”‚    manager      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚ (HTTPS API calls)
                           â–¼
                    Worker Nodes


minikube and kubectl 

minikube : is a tool that runs a single-node kubernetes cluster locally on your machine for development and testing 

why minikube : 

    instead of setting up multiple physical machine 

    master node (physical machine) + worker node 1 

    minikube creates everything in one place : 

    your laptop/desktop -> minikube VM -> complete k8s cluster 

what minikube provides : 

    control plane components : 
        api server, etcd, scheduler, controller-manager 

    worker node components : 
        kubelet, kube-proxy, container runtime 

    all in one VM or container : 

    kubectl : 

        is the command-line interface to interact with any Kubernetes cluster (local or remote)

    #kubectl is your remote control for kubernetes  
    kubectl create deployment nginx --image=nginx 
    kubectl get pods 
    kubectl delete service my-service 

    #these become HTTP requests to kube-apiserver (port 6443)



how to setup minikube 

choco install minikube 
choco install kubernetes-cli 


start minikube cluster 

#start minikube (creates a local k*s cluster)
minikube start 

#this does what "kubeadm init" does in your notes but automatically : 
# creates control plane (API server, etcd, scheduler, controller-manager)
# adds worker node componenets (kubelet, kube-proxy, containerd)
# sets up networking 
# configures kubectl to talk to this cluster 

verify setup 

#check cluster status 
minikube status 

should show :  
minikube 
type : control plane 
host : running 
kubelet : running 
apiserver : running 
kubeconfig : configured 

check nodes 
kubectl get nodes 

should show: 
name status roles age version
minikube ready control-plane 1m v1.28.3


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Your Machine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Minikube VM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                             â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€ Control Plane â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ kube-apiserver     â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ etcd               â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ kube-scheduler     â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ controller-manager â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â”‚              â”‚                               â”‚   â”‚
â”‚  â”‚              â–¼                               â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€ Worker Node â”€â”€â”€â”€â”€â”€â”                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ kubelet            â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ kube-proxy         â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ containerd         â”‚                  â”‚   â”‚
â”‚  â”‚  â”‚  â€¢ Your Pods          â”‚                  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                    â”‚
â”‚  kubectl â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚     (talks to API server via port 6443)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


basic kubectl commands for local development 

deployment commands : 

create a deployment (like fro your notes)
kubectl create deployment nginx --image=nginx --replicas=3

this triggers the same flow you documented:
kubectl -> kube-apiserver 
apiserver ->etcd (store deployment)
controller-manager creates replicaSet 
scheduler assigns pods to nodes (just one node in minikube)
kubelet starts containers 

monitoring commands 
    
    #see all resources 
    kubectl get all 

    #watches pods in real-time 
    kubectl get pods -w 

    #see detailed pod info 
    kubectl describe pod <pod-name> 

    #check logs 
    kubectl logs <pod-name> 


service & networking 

#expose deployment as service 
kubectl expose deployment nginx --port=80 --type=NodePort 

#access service in minikube 
minikube service nginx --url 
#this gives you a url like http://192.18.48.2:31234


cluster management 

minikube start //start cluster 
minikube stop 
minikube delete 
minikube pause 
minikube unpause // resume cluster 


access & debugging 

minikube dashboard  // open k8s web ui 
minikube ssh  // ssh into minikube VM 
minikube logs //see minikube ligs 


docker integration 
minikube deocker-env //use minikube's docker daemon 
eval $(minikube docker-env) //cofigure shell 


real example : deploy your docker app 

minikube start 

kubectl create deployment my-node-app --image=my-node-app --replicas=2

kubectl expose deployment my-node-app --port=300 --type=NodePort 

minikube service my-node-app --url 


Tool	    Purpose	                    Scope
minikube	Creates local K8s clusters	Local only
kubectl	    Controls ANY K8s cluster	Local + Remote





How Kubernetes Works in Layman Terms
Think of Kubernetes like a smart apartment building manager that automatically handles everything for you.

The Building Manager Analogy
Master Node = Building Manager's Office
â”Œâ”€â”€â”€ Floor 1 (Worker Node) â”€â”€â”€â” â”‚ â”‚ â”‚ ğŸ  Floor Manager (kubelet) â”‚ â”‚ "I manage this floor" â”‚ â”‚ â”‚ â”‚ ğŸ“¡ Mail Room (kube-proxy) â”‚ â”‚ "I handle deliveries" â”‚ â”‚ â”‚ â”‚ ğŸ—ï¸ Maintenance (containerd) â”‚ â”‚ "I actually build rooms" â”‚ â”‚ â”‚ â”‚ ğŸ  Room 1A (Pod) â”‚ â”‚ â””â”€ ğŸ“¦ Container â”‚ â”‚ â”‚ â”‚ ğŸ  Room 1B (Pod) â”‚ â”‚ â””â”€ ğŸ“¦ Container â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Translation: "Hey building manager, I need 3 identical apartments for my Netflix service"

Step 2: The Chain Reaction
How It Uses Your Laptop's Resources
Your laptop is essentially pretending to be a whole data center:

Resource Breakdown:
What Happens When You Run Netflix App:
Resource Usage:

CPU Spike: ~20-30% for 10 seconds (downloading image, starting containers)
RAM Usage: +300MB (3 copies Ã— 100MB each)
Disk I/O: Pulls 150MB Netflix image from internet
Network: Internal communication between components
The Complete 7-Layer Architecture
Here are ALL the layers from your hardware to your microservice:

Layer 1: Physical Hardware
Layer 2: Operating System
Layer 3: Virtualization/Container Platform
Layer 4: Kubernetes Cluster
Layer 5: Kubernetes Resources
Layer 6: Pods (Runtime Environment)
Layer 7: Your Application
Real Resource Flow Example
When someone visits your Netflix app:

Resource consumption during this:

CPU: Spikes to 40% for 2 seconds
RAM: Uses 50MB for request processing
Network: 5MB/sec video streaming
Disk: Reads movie file at 10MB/sec
Why So Many Layers?
Each layer has a specific job:

Layer	Job	Example
Hardware	Raw computing power	"I have 8 CPU cores available"
OS	Manage hardware safely	"Process A gets 2 cores, Process B gets 1"
Docker	Isolate applications	"Netflix can't see Banking app's files"
Kubernetes	Orchestrate containers	"If Netflix crashes, restart it automatically"
Resources	Define what you want	"I want 3 Netflix copies always running"
Pods	Runtime packaging	"Group Netflix app with its config files"
Application	Your actual service	"Stream movies to users"
The Magic of Microservices
Instead of one giant application, you break it into pieces:

Benefits:

Scaling: Only scale the video service when more people watch
Updates: Update recommendations without touching payments
Failures: If payments fail, videos still work
Teams: Different teams can work on different services
Your Laptop as a Mini Data Center
Kubernetes makes your laptop pretend to be Google's data center:

This lets you learn real data center skills on your personal computer!

The beauty is that the same kubectl commands that work on your laptop will work identically on a $1 million production cluster at Netflix!## Real Example: You Want to Run Netflix

Let's say you want to run a "Netflix-like" app with 3 copies for reliability:

Step 1: Your Request
Translation: "Hey building manager, I need 3 identical apartments for my Netflix service"

Step 2: The Chain Reaction
How It Uses Your Laptop's Resources
Your laptop is essentially pretending to be a whole data center:

Resource Breakdown:
What Happens When You Run Netflix App:
Resource Usage:

CPU Spike: ~20-30% for 10 seconds (downloading image, starting containers)
RAM Usage: +300MB (3 copies Ã— 100MB each)
Disk I/O: Pulls 150MB Netflix image from internet
Network: Internal communication between components
The Complete 7-Layer Architecture
Here are ALL the layers from your hardware to your microservice:

Layer 1: Physical Hardware
Layer 2: Operating System
Layer 3: Virtualization/Container Platform
Layer 4: Kubernetes Cluster
Layer 5: Kubernetes Resources
Layer 6: Pods (Runtime Environment)
Layer 7: Your Application
Real Resource Flow Example
When someone visits your Netflix app:

Resource consumption during this:

CPU: Spikes to 40% for 2 seconds
RAM: Uses 50MB for request processing
Network: 5MB/sec video streaming
Disk: Reads movie file at 10MB/sec
Why So Many Layers?
Each layer has a specific job:

Layer	Job	Example
Hardware	Raw computing power	"I have 8 CPU cores available"
OS	Manage hardware safely	"Process A gets 2 cores, Process B gets 1"
Docker	Isolate applications	"Netflix can't see Banking app's files"
Kubernetes	Orchestrate containers	"If Netflix crashes, restart it automatically"
Resources	Define what you want	"I want 3 Netflix copies always running"
Pods	Runtime packaging	"Group Netflix app with its config files"
Application	Your actual service	"Stream movies to users"
The Magic of Microservices
Instead of one giant application, you break it into pieces:

Benefits:

Scaling: Only scale the video service when more people watch
Updates: Update recommendations without touching payments
Failures: If payments fail, videos still work
Teams: Different teams can work on different services
Your Laptop as a Mini Data Center
Kubernetes makes your laptop pretend to be Google's data center:

This lets you learn real data center skills on your personal computer!

The beauty is that the same kubectl commands that work on your laptop will work identically on a $1 million production cluster at Netflix!

Similar code found with 2 license types - View matches








when you install kubernetes : 

    you install a control plane on one machine(or more, for high availability)
    you install a kubelet agent on each of the other machines 
    you join all worker machines (nodes) to the control plane

    that makes one kubernetes cluster 

install kubernetes and a container runtime on all machines 
    you need tools like 
        kubelet (the agent that talks to the master)
        kubeadm (for initializing the cluster)
        containerd or docker (to actually run containers)
    
one master node 
    initialize the cluster 
        kubeadm init 
    
    this sets up : 

        api server 
        etcd (cluster start datebase)
        scheduler 
        controller manager 
    
    it also gives you a join token like:

    kubeadm join <master-ip>:6443 --token abcdef.12345124 ..

on worker nodes 

    kubeadm join <master-ip>:6443 --token ...

    this resisters the machine to the cluster

    now this worker machine becomes a Node in kubernetes!


once joined, what happens ? 

    the master can now schedule pods(containers) on any node 
    each node runs 
        kubelet - to report health, run pods 
        containerd - to run containers 
    
    all nodes talk to the control plane via HTTPS (port 6443)


                 Kubernetes Cluster
        +---------------- Control Plane --------------+
        |                                             |
        |   kube-apiserver                            |
        |   scheduler                                 |
        |   controller-manager                        |
        |   etcd (cluster database)                   |
        +------------------+--------------------------+
                           |
                           | (API Calls via 6443)
                           â†“
        +---------------------------------------------+
        |                 Node 1 (Worker)             |
        |   - kubelet                                 |
        |   - containerd (or Docker)                  |
        |   - runs Pods                               |
        +---------------------------------------------+
                           â†“
        +---------------------------------------------+
        |                 Node 2 (Worker)             |
        |   - kubelet                                 |
        |   - containerd                              |
        |   - runs Pods                               |
        +---------------------------------------------+


cluster : all the machines together(master + nodes)
node : one physical or virtual machine (worker)
pod : smallest unit in kubernetes (hold containers)
kubelet : agetn running on each node to talk to the master 
scheduler : decides where to place pods (on which node)
etcd : database that stores cluster state (key-value store)


how master node makes decisions, or how pods get scheduled 
and communicate accross machines (networking)?

kubernetes master node runs a scheduler, which decides : 
    which node should run a pod?

    based on : 
        available CPU, memory
        Taints, labels, node selectors
        affinity rules 
        past usage, constraints etc 

    
    components involved in scheduling : 

        kube-apiserver : entry point receives all pod creation requests 
        etcd : distributed key-value store (stores curretn state of the cluster)
        kube-scheduler : decides where to run the pod 
        kube-controller-manager : ensure the desired state in maintained (eg. if a node goes down, it reschedules pods)
        kubelet(on node) : pulls the pods spec from the api server and runs it using containerd     
    
